{"cells":[{"cell_type":"markdown","metadata":{"id":"cZzv73_UWb6N"},"source":["# Bagging and Pasting\n","\n","#### Part of the [Inquiryum Machine Learning Fundamentals Course](http://inquiryum.com/machine-learning/)\n","\n","![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/divider.png)\n","\n","![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/bagging..png)\n","\n","\n","Now we are about to embark on our journey from simple decision trees to algorithms that use decision trees as components. The path goes like this:\n","\n","\n","![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/dbxg.png)\n","\n","The use of decision trees began in the 1980s and XGBoost was introduced in 2016. Throughout the next few notebooks we will explore this progression of algorithms.  \n","\n","### A collective of classifiers\n","\n","To gain an intuition on how this works, let's look at how our confidence might increase when more people tell us something. Whether it is multiple doctors giving us the same diagnosis or something as simple as ...\n","\n","#### The Mary Spender example\n","\n","Let's say one of your friends mentions over lunch that you would love a particular musical artist on YouTube, say Mary Spender, who you never heard before. \n","\n","![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/MarySpender2.png)\n","\n","What is the chance that you will actually like Mary Spender's music? Maybe slighly better than chance? Let's say you think there is a 60% chance you will like her. You will file away the recommendation but you are not going to rush home and watch a YouTube video.  Now, in addition to the lunch friend's recommendation,  an old music school friend, now living in Austin messages you saying you should check out Mary Spender and the friend predicts you will absolutely love her. Then a week later, while talking with an old bandmate over the phone, that bandmate, again, recommends Mary Spender. Over the course of less than 10 days, three of your friends independently (because they don't know one another) recommend Mary Spender. Now what is the likelihood of you liking Mary Spender? I am guessing you think that now it is higher than 60%. Maybe now you think it is 90% likely you will like her. It is the aggregate of these 3 people's opinions (3 classifiers) that ups the accuracy of the prediction.\n","\n","![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/spender22.png)\n","\n","\n","This is similar to how bagging works. One aggregates the votes of a number of classifiers and the vote of that ensemble of classifiers is more accurate than that of a single classifier. Even if the accuracy of each component classifier is low (known as a weak classifier), the ensemble can be a strong (high accuracy) classifier. Of course there are some caveats. \n","\n","Back to the Mary Spender example. Suppose one of your friends went to a Mary Spender concert and then later in the week met with four other of your friends and mentioned that she thought you would love Mary Spender's music. Then, over the course of a week all those friends recommended Mary Spender to you. In this case the recommendations are not that independent---all are based on one person's opinion. Thus, the accuracy would not be as great as in the example above. Similarly, if you made 10 copies of the exact same classifier each trained on exactly the same data, the accuracy of the ensemble of clones would not be any better than the accuracy of a single copy. Moving away from Mary Spender and our musical tastes and back to machine learning, we can try to create independence among the classifier in 2 ways:\n","\n","1. We can change the type of classifier. For example, we can use a k-Nearest Neighbor Classifier with Manhattan distance and a k of 5, a k-Nearest Neighbor Classifier with Euclidean distance and a k of 3, a decision tree classifier using entropy and a max depth of 5, and a decision tree classifier with using gini and no max depth specified. Hopefully, the accuracy of the ensemble of the four classifiers would be greater than that of a single classifier.\n","2. We can have an ensemble of the same classifier (for example, 10 decision tree classifiers with identical hyperparameters) but each classifier can get a different subset of the training data. The classifiers would thus build different models (differents 'rules') and, again, the accuracy of the ensemble should be greather than that of a single classifier. This is the approach we will take.\n","\n","### Bagging and Pasting\n","\n","\n","In this Jupyter notebook, we are going to explore Bagging algorithms. Bagging algorithms come in a variety of 'flavors' including one called 'bagging' and one called 'pasting'.\n","\n","But first an experiment on the what *with replacement* means. As you will see shortly, that term is the crucial difference between bagging and pasting.\n","\n","### A small experiment\n","NOTE: The following code is just used for illustration and is nothing we will be using for machine learning. \n","\n","Consider a list of 5 red balls and 5 blue balls:"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"xkhfXZatWb6S","executionInfo":{"status":"ok","timestamp":1679282186298,"user_tz":240,"elapsed":155,"user":{"displayName":"stephano montes","userId":"15446985780563685802"}}},"outputs":[],"source":["bag = ['red', 'red', 'red', 'red', 'red',\n","       'blue', 'blue', 'blue', 'blue', 'blue']"]},{"cell_type":"markdown","metadata":{"id":"ugElXnm3Wb6T"},"source":["Suppose we want to pick 7 random balls from this list. Python offers two functions that will give us random elements from a list.One is called `choices` which selects a sample with replacement, which means that once a ball is selected it is put back in the bag so it has the potential to be selected again. Let's give it a try, and just because things are random let's do this 100 times:"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"u6f_BpdyWb6T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679282187511,"user_tz":240,"elapsed":142,"user":{"displayName":"stephano montes","userId":"15446985780563685802"}},"outputId":"366dd1c0-5422-49cd-c36b-33eae861815e"},"outputs":[{"output_type":"stream","name":"stdout","text":["1 blue and 6 red\n","1 blue and 6 red\n","1 blue and 6 red\n","1 blue and 6 red\n","1 blue and 6 red\n","6 blue and 1 red\n","1 blue and 6 red\n","1 blue and 6 red\n","6 blue and 1 red\n","6 blue and 1 red\n","1 blue and 6 red\n","6 blue and 1 red\n","6 blue and 1 red\n","1 blue and 6 red\n","1 blue and 6 red\n","Balls selected exceeded balls in bag: 15\n"]}],"source":["import random\n","total = 0\n","for i in range(100):\n","    set = random.choices(bag, k=7)\n","    blue = set.count('blue')\n","    red = set.count('red')\n","    if blue > 5 or red > 5:\n","        print(\"%i blue and %i red\" % (set.count('blue'), set.count('red')))\n","        total +=1\n","print(\"Balls selected exceeded balls in bag: %i\" % (total))"]},{"cell_type":"markdown","metadata":{"id":"IxWdBvoGWb6U"},"source":["*A reminder: please don't mindlessly execute the code. Look at it and understand it*\n","\n","There are five blue balls. Since we are doing the selection with replacement there are times when we select more than 5 blue balls (or five red ones).  \n","\n","When we print\n","\n","```\n","Balls selected exceeded balls in bag: 13\n","```\n","\n","it shows how many times that was the case.\n","\n","\n","When I ran this, 14 times out of 100 had more of one color ball than there were in the original bag. In fact, several times I ended up with all 7 of the balls blue, even though the original list had only 5 balls:\n","\n","```\n","7 blue and 0 red\n","6 blue and 1 red\n","7 blue and 0 red\n","1 blue and 6 red\n","0 blue and 7 red\n","1 blue and 6 red\n","1 blue and 6 red\n","6 blue and 1 red\n","6 blue and 1 red\n","1 blue and 6 red\n","6 blue and 1 red\n","1 blue and 6 red\n","6 blue and 1 red\n","6 blue and 1 red\n","Balls selected exceeded balls in bag: 14\n","```\n","Again, this is called selecting with replacement (we put what we selected back in the set before selecting again). \n","\n","The other alternative is to select without replacement--once we select something we can't select it again. Python's `sample` does this:\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"wjTz87jbWb6U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679282189341,"user_tz":240,"elapsed":2,"user":{"displayName":"stephano montes","userId":"15446985780563685802"}},"outputId":"541c330b-c7d9-4adf-836b-376be4786425"},"outputs":[{"output_type":"stream","name":"stdout","text":["Balls selected exceeded balls in bag: 0\n"]}],"source":["import random\n","total = 0\n","for i in range(1000):\n","    set = random.sample(bag, k=7)\n","    blue = set.count('blue')\n","    red = set.count('red')\n","    if blue > 5 or red > 5:\n","        print(\"%i blue and %i red\" % (set.count('blue'), set.count('red')))\n","        total +=1\n","print(\"Balls selected exceeded balls in bag: %i\" % (total))"]},{"cell_type":"markdown","metadata":{"id":"N-RBRpyMWb6U"},"source":["As you can see, the number of a specific colored ball that we select never exceeded the number of balls of that color in the original set.\n","\n","Now back to bagging and pasting. In both approaches we are going to sample the training data. Let's say we want 70% of the training data in our sample. In bagging ([Breiman, 1996](https://link.springer.com/content/pdf/10.1007/BF00058655.pdf)), if we our training dataset is 1000 instances and we want 70% for a particular classifier, the algorithm will randomly select 700 out of the 1,000 **with replacement**. With pasting ([Breiman, 1998](https://link.springer.com/article/10.1023/A:1007563306331)), the selection is done **without replacement**. \n","\n","#### but wait, there is more ...\n","\n","There are two other options. Instead of selecting a random subset of training data instances, we can select a random subset of columns (features). Let's say we have a dataset of 1,000 instances each with 100 features. When we select a random subset of columns, we still have 1,000 instances but now they have just a subset of the features. This is called Random Subspaces ([Ho, 1998](https://pdfs.semanticscholar.org/b41d/0fa5fdaadd47fc882d3db04277d03fb21832.pdf?_ga=2.196949164.1638238666.1596910000-1073138517.1596910000)).\n","\n","Finally, we can train a classifier on both random subsets of instances and random subsets of features. This is known as Random Patches ([Louppe and Geurts, 2012](https://www.researchgate.net/publication/262212941_Ensembles_on_Random_Patches))\n","\n","In summary, the four methods are:\n","\n","* **bagging** - select a subset of data set instances using replacement\n","* **pasting** - select a subset of data set instances without replacement\n","* **Random Subspaces** - select a subset of features\n","* **Random Patches** - select both a subset of features and of instances\n","\n","Let's see how this works!\n","\n","First, let's grab the Wisconsin Cancer data we used before:\n","\n","#### Wisconsin Cancer Dataset\n","![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/aimam.png)\n","image from Nvidia's [AI Improves Breast Cancer Diagnoses by Factoring Out False Positives](https://blogs.nvidia.com/blog/2018/02/01/making-mammography-more-meaningful/)\n","\n","[A description of the Cancer Database](#Breast-Cancer-Database)\n","\n","In this dataset we are trying to predict the diagnosis---either M (malignant) or B (benign).\n","\n","Let's load the dataset and split it into training and testing sets"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"WMWJytA6Wb6W","colab":{"base_uri":"https://localhost:8080/","height":519},"executionInfo":{"status":"ok","timestamp":1679282192833,"user_tz":240,"elapsed":1291,"user":{"displayName":"stephano montes","userId":"15446985780563685802"}},"outputId":"fd1703d0-12cc-4e6f-9303-24820912fe83"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["          diagnosis  radiusAvg  textureAvg  perimeterAvg  areaAvg  \\\n","id                                                                  \n","904647            B     11.930       10.91         76.14    442.7   \n","863270            B     12.360       18.54         79.01    466.7   \n","88147202          B     12.620       23.97         81.35    496.4   \n","881972            M     17.050       19.08        113.40    895.0   \n","8712853           B     14.970       16.95         96.22    685.9   \n","...             ...        ...         ...           ...      ...   \n","8510824           B      9.504       12.44         60.34    273.9   \n","898143            B      9.606       16.84         61.64    280.5   \n","90769602          B     12.720       17.67         80.98    501.3   \n","911320502         B     13.170       18.22         84.28    537.3   \n","903483            B      8.734       16.84         55.27    234.3   \n","\n","           smoothnessAvg  compactnessAvg  concavityAvg  concavityPointsAvg  \\\n","id                                                                           \n","904647           0.08872         0.05242       0.02606             0.01796   \n","863270           0.08477         0.06815       0.02643             0.01921   \n","88147202         0.07903         0.07529       0.05438             0.02036   \n","881972           0.11410         0.15720       0.19100             0.10900   \n","8712853          0.09855         0.07885       0.02602             0.03781   \n","...                  ...             ...           ...                 ...   \n","8510824          0.10240         0.06492       0.02956             0.02076   \n","898143           0.08481         0.09228       0.08422             0.02292   \n","90769602         0.07896         0.04522       0.01402             0.01835   \n","911320502        0.07466         0.05994       0.04859             0.02870   \n","903483           0.10390         0.07428       0.00000             0.00000   \n","\n","           symmetryAvg  ...  radiusWorst  textureWorst  perimeterWorst  \\\n","id                      ...                                              \n","904647          0.1601  ...        13.80         20.14           87.64   \n","863270          0.1602  ...        13.29         27.49           85.56   \n","88147202        0.1514  ...        14.20         31.31           90.67   \n","881972          0.2131  ...        19.59         24.89          133.50   \n","8712853         0.1780  ...        16.11         23.00          104.60   \n","...                ...  ...          ...           ...             ...   \n","8510824         0.1815  ...        10.23         15.66           65.13   \n","898143          0.2036  ...        10.75         23.07           71.25   \n","90769602        0.1459  ...        13.82         20.96           88.87   \n","911320502       0.1454  ...        14.90         23.89           95.10   \n","903483          0.1985  ...        10.17         22.80           64.01   \n","\n","           areaWorst  smoothnessWorst  compactnessWorst  concavityWorst  \\\n","id                                                                        \n","904647         589.5           0.1374           0.15750         0.15140   \n","863270         544.1           0.1184           0.19630         0.19370   \n","88147202       624.0           0.1227           0.34540         0.39110   \n","881972        1189.0           0.1703           0.39340         0.50180   \n","8712853        793.7           0.1216           0.16370         0.06648   \n","...              ...              ...               ...             ...   \n","8510824        314.9           0.1324           0.11480         0.08867   \n","898143         353.6           0.1233           0.34160         0.43410   \n","90769602       586.8           0.1068           0.09605         0.03469   \n","911320502      687.6           0.1282           0.19650         0.18760   \n","903483         317.0           0.1460           0.13100         0.00000   \n","\n","           concavityPointsWorst  symmetryWorst>  FractalDimensionWorst  \n","id                                                                      \n","904647                  0.06876          0.2460                0.07262  \n","863270                  0.08442          0.2983                0.07185  \n","88147202                0.11800          0.2826                0.09585  \n","881972                  0.25430          0.3109                0.09061  \n","8712853                 0.08485          0.2404                0.06428  \n","...                         ...             ...                    ...  \n","8510824                 0.06227          0.2450                0.07773  \n","898143                  0.08120          0.2982                0.09825  \n","90769602                0.03612          0.2165                0.06025  \n","911320502               0.10450          0.2235                0.06925  \n","903483                  0.00000          0.2445                0.08865  \n","\n","[114 rows x 31 columns]"],"text/html":["\n","  <div id=\"df-8e6cd0bf-be1a-40c1-a77b-326fe8514314\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>diagnosis</th>\n","      <th>radiusAvg</th>\n","      <th>textureAvg</th>\n","      <th>perimeterAvg</th>\n","      <th>areaAvg</th>\n","      <th>smoothnessAvg</th>\n","      <th>compactnessAvg</th>\n","      <th>concavityAvg</th>\n","      <th>concavityPointsAvg</th>\n","      <th>symmetryAvg</th>\n","      <th>...</th>\n","      <th>radiusWorst</th>\n","      <th>textureWorst</th>\n","      <th>perimeterWorst</th>\n","      <th>areaWorst</th>\n","      <th>smoothnessWorst</th>\n","      <th>compactnessWorst</th>\n","      <th>concavityWorst</th>\n","      <th>concavityPointsWorst</th>\n","      <th>symmetryWorst&gt;</th>\n","      <th>FractalDimensionWorst</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>904647</th>\n","      <td>B</td>\n","      <td>11.930</td>\n","      <td>10.91</td>\n","      <td>76.14</td>\n","      <td>442.7</td>\n","      <td>0.08872</td>\n","      <td>0.05242</td>\n","      <td>0.02606</td>\n","      <td>0.01796</td>\n","      <td>0.1601</td>\n","      <td>...</td>\n","      <td>13.80</td>\n","      <td>20.14</td>\n","      <td>87.64</td>\n","      <td>589.5</td>\n","      <td>0.1374</td>\n","      <td>0.15750</td>\n","      <td>0.15140</td>\n","      <td>0.06876</td>\n","      <td>0.2460</td>\n","      <td>0.07262</td>\n","    </tr>\n","    <tr>\n","      <th>863270</th>\n","      <td>B</td>\n","      <td>12.360</td>\n","      <td>18.54</td>\n","      <td>79.01</td>\n","      <td>466.7</td>\n","      <td>0.08477</td>\n","      <td>0.06815</td>\n","      <td>0.02643</td>\n","      <td>0.01921</td>\n","      <td>0.1602</td>\n","      <td>...</td>\n","      <td>13.29</td>\n","      <td>27.49</td>\n","      <td>85.56</td>\n","      <td>544.1</td>\n","      <td>0.1184</td>\n","      <td>0.19630</td>\n","      <td>0.19370</td>\n","      <td>0.08442</td>\n","      <td>0.2983</td>\n","      <td>0.07185</td>\n","    </tr>\n","    <tr>\n","      <th>88147202</th>\n","      <td>B</td>\n","      <td>12.620</td>\n","      <td>23.97</td>\n","      <td>81.35</td>\n","      <td>496.4</td>\n","      <td>0.07903</td>\n","      <td>0.07529</td>\n","      <td>0.05438</td>\n","      <td>0.02036</td>\n","      <td>0.1514</td>\n","      <td>...</td>\n","      <td>14.20</td>\n","      <td>31.31</td>\n","      <td>90.67</td>\n","      <td>624.0</td>\n","      <td>0.1227</td>\n","      <td>0.34540</td>\n","      <td>0.39110</td>\n","      <td>0.11800</td>\n","      <td>0.2826</td>\n","      <td>0.09585</td>\n","    </tr>\n","    <tr>\n","      <th>881972</th>\n","      <td>M</td>\n","      <td>17.050</td>\n","      <td>19.08</td>\n","      <td>113.40</td>\n","      <td>895.0</td>\n","      <td>0.11410</td>\n","      <td>0.15720</td>\n","      <td>0.19100</td>\n","      <td>0.10900</td>\n","      <td>0.2131</td>\n","      <td>...</td>\n","      <td>19.59</td>\n","      <td>24.89</td>\n","      <td>133.50</td>\n","      <td>1189.0</td>\n","      <td>0.1703</td>\n","      <td>0.39340</td>\n","      <td>0.50180</td>\n","      <td>0.25430</td>\n","      <td>0.3109</td>\n","      <td>0.09061</td>\n","    </tr>\n","    <tr>\n","      <th>8712853</th>\n","      <td>B</td>\n","      <td>14.970</td>\n","      <td>16.95</td>\n","      <td>96.22</td>\n","      <td>685.9</td>\n","      <td>0.09855</td>\n","      <td>0.07885</td>\n","      <td>0.02602</td>\n","      <td>0.03781</td>\n","      <td>0.1780</td>\n","      <td>...</td>\n","      <td>16.11</td>\n","      <td>23.00</td>\n","      <td>104.60</td>\n","      <td>793.7</td>\n","      <td>0.1216</td>\n","      <td>0.16370</td>\n","      <td>0.06648</td>\n","      <td>0.08485</td>\n","      <td>0.2404</td>\n","      <td>0.06428</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>8510824</th>\n","      <td>B</td>\n","      <td>9.504</td>\n","      <td>12.44</td>\n","      <td>60.34</td>\n","      <td>273.9</td>\n","      <td>0.10240</td>\n","      <td>0.06492</td>\n","      <td>0.02956</td>\n","      <td>0.02076</td>\n","      <td>0.1815</td>\n","      <td>...</td>\n","      <td>10.23</td>\n","      <td>15.66</td>\n","      <td>65.13</td>\n","      <td>314.9</td>\n","      <td>0.1324</td>\n","      <td>0.11480</td>\n","      <td>0.08867</td>\n","      <td>0.06227</td>\n","      <td>0.2450</td>\n","      <td>0.07773</td>\n","    </tr>\n","    <tr>\n","      <th>898143</th>\n","      <td>B</td>\n","      <td>9.606</td>\n","      <td>16.84</td>\n","      <td>61.64</td>\n","      <td>280.5</td>\n","      <td>0.08481</td>\n","      <td>0.09228</td>\n","      <td>0.08422</td>\n","      <td>0.02292</td>\n","      <td>0.2036</td>\n","      <td>...</td>\n","      <td>10.75</td>\n","      <td>23.07</td>\n","      <td>71.25</td>\n","      <td>353.6</td>\n","      <td>0.1233</td>\n","      <td>0.34160</td>\n","      <td>0.43410</td>\n","      <td>0.08120</td>\n","      <td>0.2982</td>\n","      <td>0.09825</td>\n","    </tr>\n","    <tr>\n","      <th>90769602</th>\n","      <td>B</td>\n","      <td>12.720</td>\n","      <td>17.67</td>\n","      <td>80.98</td>\n","      <td>501.3</td>\n","      <td>0.07896</td>\n","      <td>0.04522</td>\n","      <td>0.01402</td>\n","      <td>0.01835</td>\n","      <td>0.1459</td>\n","      <td>...</td>\n","      <td>13.82</td>\n","      <td>20.96</td>\n","      <td>88.87</td>\n","      <td>586.8</td>\n","      <td>0.1068</td>\n","      <td>0.09605</td>\n","      <td>0.03469</td>\n","      <td>0.03612</td>\n","      <td>0.2165</td>\n","      <td>0.06025</td>\n","    </tr>\n","    <tr>\n","      <th>911320502</th>\n","      <td>B</td>\n","      <td>13.170</td>\n","      <td>18.22</td>\n","      <td>84.28</td>\n","      <td>537.3</td>\n","      <td>0.07466</td>\n","      <td>0.05994</td>\n","      <td>0.04859</td>\n","      <td>0.02870</td>\n","      <td>0.1454</td>\n","      <td>...</td>\n","      <td>14.90</td>\n","      <td>23.89</td>\n","      <td>95.10</td>\n","      <td>687.6</td>\n","      <td>0.1282</td>\n","      <td>0.19650</td>\n","      <td>0.18760</td>\n","      <td>0.10450</td>\n","      <td>0.2235</td>\n","      <td>0.06925</td>\n","    </tr>\n","    <tr>\n","      <th>903483</th>\n","      <td>B</td>\n","      <td>8.734</td>\n","      <td>16.84</td>\n","      <td>55.27</td>\n","      <td>234.3</td>\n","      <td>0.10390</td>\n","      <td>0.07428</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.1985</td>\n","      <td>...</td>\n","      <td>10.17</td>\n","      <td>22.80</td>\n","      <td>64.01</td>\n","      <td>317.0</td>\n","      <td>0.1460</td>\n","      <td>0.13100</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.2445</td>\n","      <td>0.08865</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>114 rows Ã— 31 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8e6cd0bf-be1a-40c1-a77b-326fe8514314')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-8e6cd0bf-be1a-40c1-a77b-326fe8514314 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-8e6cd0bf-be1a-40c1-a77b-326fe8514314');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","colNames = ['id', 'diagnosis', 'radiusAvg', 'textureAvg', 'perimeterAvg', 'areaAvg',\n","            'smoothnessAvg', 'compactnessAvg', 'concavityAvg', 'concavityPointsAvg',\n","            'symmetryAvg', 'FractalDimensionAvg', 'radiusSE', 'textureSE', 'perimeterSE',\n","            'areaSE','smoothnessSE', 'compactnessSE', 'concavitySE', 'concavityPointsSE',\n","            'symmetrySE', 'FractalDimensionSE', 'radiusWorst', 'textureWorst', 'perimeterWorst',\n","            'areaWorst', 'smoothnessWorst', 'compactnessWorst', 'concavityWorst', 'concavityPointsWorst',\n","            'symmetryWorst>', 'FractalDimensionWorst']\n","len(colNames)\n","\n","data = pd.read_csv('https://raw.githubusercontent.com/zacharski/ml-class/master/data/wdbc.data', names=colNames)\n","data.set_index('id', inplace=True)\n","\n","trainingdata, testdata = train_test_split(data, test_size = 0.2)\n","testdata"]},{"cell_type":"markdown","metadata":{"id":"TEQMMnQEWb6b"},"source":["Now divide up the data into the features and labels"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"BUFu_lp1Wb6b","executionInfo":{"status":"ok","timestamp":1679282192833,"user_tz":240,"elapsed":2,"user":{"displayName":"stephano montes","userId":"15446985780563685802"}}},"outputs":[],"source":["colNames.remove('id')\n","colNames.remove('diagnosis')\n","trainingDataFeatures = trainingdata[colNames]\n","testDataFeatures = testdata[colNames]\n","trainingDataLabels = trainingdata['diagnosis']\n","testDataLabels = testdata['diagnosis']\n"]},{"cell_type":"markdown","metadata":{"id":"9jwhHLytWb6c"},"source":["Let's get a base accuracy using a single decision tree classifier:"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"0451MpM0Wb6d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679282194356,"user_tz":240,"elapsed":285,"user":{"displayName":"stephano montes","userId":"15446985780563685802"}},"outputId":"023e173c-7a47-4775-c393-017af912fbaa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9473684210526315"]},"metadata":{},"execution_count":6}],"source":["from sklearn import tree\n","clf = tree.DecisionTreeClassifier(criterion='entropy')\n","clf.fit(trainingDataFeatures, trainingDataLabels)\n","predictions = clf.predict(testDataFeatures)\n","\n","accuracy_score(testDataLabels, predictions)"]},{"cell_type":"markdown","metadata":{"id":"asjigQIyWb6d"},"source":["Now we will see if we can improve on that accuracy.\n","\n","### Building a bagging classifier\n","\n","Let's build a collective of 20 decision tree classifiers (`n_estimators`). Let's train each one with 100 random samples from our dataset (`max_samples`) with replacement (`bootstrap=True`). `n_jobs` means how many jobs to run in parallel. `n_jobs=-1` means use all available CPU cores.   \n","\n","Just to reinforce the vocabulary we are learning, `n_estimators`, `max_samples`, `bootstrap` are among the **hyperparameters** of the bagging classifier."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"tL_K6UxpWb6d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679282197019,"user_tz":240,"elapsed":1055,"user":{"displayName":"stephano montes","userId":"15446985780563685802"}},"outputId":"e3475817-273e-4c48-bfec-f98d9fea4f34"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9473684210526315"]},"metadata":{},"execution_count":7}],"source":["from sklearn.ensemble import BaggingClassifier\n","clf = tree.DecisionTreeClassifier(criterion='entropy')\n","\n","bagging_clf = BaggingClassifier(clf, n_estimators=20, max_samples=100, \n","                                bootstrap=True, n_jobs=-1)\n","bagging_clf.fit(trainingDataFeatures, trainingDataLabels)\n","predictions = bagging_clf.predict(testDataFeatures)\n","accuracy_score(testDataLabels, predictions)"]},{"cell_type":"markdown","metadata":{"id":"E1XYY6-2Wb6d"},"source":["When I did this using a single decision tree classifier was 90.3% accurate, while the bagging classifier was 96.5% accurate--halving the error rate! that's pretty good!\n","\n","\n","### Pasting\n","Let's try the same thing with pasting (without replacement):\n","\n","For that we set the hyperparameter: `bootstrap=False`\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"YIwO0j1xWb6e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679282199950,"user_tz":240,"elapsed":117,"user":{"displayName":"stephano montes","userId":"15446985780563685802"}},"outputId":"79f45c53-3ff0-4a10-bfba-93e1aa008dd7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9298245614035088"]},"metadata":{},"execution_count":8}],"source":["pasting_clf = BaggingClassifier(clf, n_estimators=20, max_samples=100, \n","                                bootstrap=False, n_jobs=-1)\n","pasting_clf.fit(trainingDataFeatures, trainingDataLabels)\n","predictions = pasting_clf.predict(testDataFeatures)\n","accuracy_score(testDataLabels, predictions)"]},{"cell_type":"markdown","metadata":{"id":"EcCz_Gu_Wb6f"},"source":["### Random Subspaces\n","Again, random subspaces are when we randomly select feature subsets rather than subsets of the dataset instances. This time we will create 50 classifiers for our collective and each will train on a dataset with 7 features (`max_feature=7`)."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"iCf10iICWb6f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679282201195,"user_tz":240,"elapsed":294,"user":{"displayName":"stephano montes","userId":"15446985780563685802"}},"outputId":"13413599-0425-4597-f85f-a77bc9105f45"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.956140350877193"]},"metadata":{},"execution_count":9}],"source":["subspace_clf = BaggingClassifier(clf, n_estimators=50, max_features=7, \n","                                bootstrap=True, n_jobs=-1)\n","subspace_clf.fit(trainingDataFeatures, trainingDataLabels)\n","predictions = subspace_clf.predict(testDataFeatures)\n","accuracy_score(testDataLabels, predictions)"]},{"cell_type":"markdown","metadata":{"id":"_-9qdIG-Wb6f"},"source":["### Random Patches\n","Finally, let's combine things and try random patches. In this example each classifier will be given a subset of 100 training instances with 7 features each.:"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"JCjPbM08Wb6g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679282202606,"user_tz":240,"elapsed":276,"user":{"displayName":"stephano montes","userId":"15446985780563685802"}},"outputId":"a522be86-df3e-4505-cb0c-cbc43b297861"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.956140350877193"]},"metadata":{},"execution_count":10}],"source":["subspace_clf = BaggingClassifier(clf, n_estimators=100, max_features=7, \n","                                 max_samples=100, bootstrap=False, n_jobs=-1)\n","subspace_clf.fit(trainingDataFeatures, trainingDataLabels)\n","predictions = subspace_clf.predict(testDataFeatures)\n","accuracy_score(testDataLabels, predictions)"]},{"cell_type":"markdown","metadata":{"id":"8bD3S8xDWb6g"},"source":["While it is common to use a decision tree as the base classifier, we can use any classifier. Here we use kNN:"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"kuTZAjpIWb6g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679282203864,"user_tz":240,"elapsed":123,"user":{"displayName":"stephano montes","userId":"15446985780563685802"}},"outputId":"a2bae375-0357-468a-bdb8-e24e65bd052c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9473684210526315"]},"metadata":{},"execution_count":11}],"source":["from sklearn.neighbors import KNeighborsClassifier\n","kNN = KNeighborsClassifier()\n","bagging_clf = BaggingClassifier(kNN, n_estimators=20, max_samples=100, \n","                                bootstrap=True, n_jobs=-1)\n","bagging_clf.fit(trainingDataFeatures, trainingDataLabels)\n","predictions = bagging_clf.predict(testDataFeatures)\n","accuracy_score(testDataLabels, predictions)"]},{"cell_type":"markdown","metadata":{"id":"4_av0jdqWb6g"},"source":["#### Summary\n","As you can see, any of these simple bagging algorithms typically outperforms using a single classifier. \n","\n","\n","### Review\n","\n","We import the bagging classifier library with:\n","\n","```\n","from sklearn.ensemble import BaggingClassifier\n","```\n","\n","and create an instance of one with:\n","\n","```\n","my_bagging_classifier = BaggingClassifier(baseClassifier, Hyperparameters,n_jobs=-1)\n","```\n","\n","#### Base Classifier\n","while any classifier can be used we typically use a decision tree\n","\n","#### Hyperparameters\n","Here is a list of the hyperparameters (from the [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier)):\n","\n","* `n_estimators`: integer, default value = 10, the number of classifiers (estimators) in the ensemble.\n","* `max_samples`: integer or float, default value = 1.0(meaning use all the training instances), the number of samples (instances) to draw from the training dataset to train each base classifier.\n","    * if integer, then draw max_features features.\n","    * if float, then draw max_samples * X.shape[0] samples. For example if `max_samples` is 0.7 and there are 100 instances in the training dataset then draw 70 samples.\n","* `max_features`, integer or float, default value =1.0,  \n","the number of features to draw from the training dataset to train each base estimator \n","    * if integers, then draw max_features features.\n","    * if float, then draw max_features * X.shape[1] features.\n","* `bootstrap` boolean, default value =True, whether samples and features are drawn with replacement. If False, sampling without replacement is performed.\n","\n","For other hyperparamters, consult the documentation.\n","\n","\n","![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/torchdivide.png)\n","\n","\n","# <font color='#EE4C2C'>You Try ...</font> \n","## <font color='#EE4C2C'>Predicting musical genres from audio file attributes</font> \n","\n","\n","When you listen even to a few seconds of a song you can identify it as blues, country, classical, or any other genre. How do you do this? What attributes are you hearing in the audio file that helps you make this classification? And, more to the point, can we train a computer to do it?\n","\n","![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/bluesClassical.png)\n","\n","We are going to be using  the [GTZAN Dataset for Music Genre Classification](https://www.kaggle.com/andradaolteanu/gtzan-dataset-music-genre-classification). It provides data of 100 songs for each of 10 genres. The data is in several formats:\n","\n","* 30 second audio files (wav)\n","* spectral images of those 30 second clips (see image above)\n","* a csv file containing acoustic attributes of the 30 second clip\n","* a csv file containing acoustic attributes of 3 second clips (the 30 second clips were split into 3 second ones)\n","\n","We are going to use the 3 second csv file which is available at \n","\n","https://raw.githubusercontent.com/zacharski/ml-class/master/data/gtzan.csv\n","\n","Go ahead and load the data into a dataframe (the first row contains feature names)\n"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"IiF0cRc4Wb6h","colab":{"base_uri":"https://localhost:8080/","height":488},"executionInfo":{"status":"ok","timestamp":1679283449726,"user_tz":240,"elapsed":665,"user":{"displayName":"stephano montes","userId":"15446985780563685802"}},"outputId":"f5b68980-1b2e-4ffc-a48c-b5f635653e23"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["               filename  length  chroma_stft_mean  chroma_stft_var  rms_mean  \\\n","0     blues.00000.0.wav   66149          0.335406         0.091048  0.130405   \n","1     blues.00000.1.wav   66149          0.343065         0.086147  0.112699   \n","2     blues.00000.2.wav   66149          0.346815         0.092243  0.132003   \n","3     blues.00000.3.wav   66149          0.363639         0.086856  0.132565   \n","4     blues.00000.4.wav   66149          0.335579         0.088129  0.143289   \n","...                 ...     ...               ...              ...       ...   \n","9985   rock.00099.5.wav   66149          0.349126         0.080515  0.050019   \n","9986   rock.00099.6.wav   66149          0.372564         0.082626  0.057897   \n","9987   rock.00099.7.wav   66149          0.347481         0.089019  0.052403   \n","9988   rock.00099.8.wav   66149          0.387527         0.084815  0.066430   \n","9989   rock.00099.9.wav   66149          0.369293         0.086759  0.050524   \n","\n","       rms_var  spectral_centroid_mean  spectral_centroid_var  \\\n","0     0.003521             1773.065032          167541.630869   \n","1     0.001450             1816.693777           90525.690866   \n","2     0.004620             1788.539719          111407.437613   \n","3     0.002448             1655.289045          111952.284517   \n","4     0.001701             1630.656199           79667.267654   \n","...        ...                     ...                    ...   \n","9985  0.000097             1499.083005          164266.886443   \n","9986  0.000088             1847.965128          281054.935973   \n","9987  0.000701             1346.157659          662956.246325   \n","9988  0.000320             2084.515327          203891.039161   \n","9989  0.000067             1634.330126          411429.169769   \n","\n","      spectral_bandwidth_mean  spectral_bandwidth_var  ...  mfcc16_var  \\\n","0                 1972.744388           117335.771563  ...   39.687145   \n","1                 2010.051501            65671.875673  ...   64.748276   \n","2                 2084.565132            75124.921716  ...   67.336563   \n","3                 1960.039988            82913.639269  ...   47.739452   \n","4                 1948.503884            60204.020268  ...   30.336359   \n","...                       ...                     ...  ...         ...   \n","9985              1718.707215            85931.574523  ...   42.485981   \n","9986              1906.468492            99727.037054  ...   32.415203   \n","9987              1561.859087           138762.841945  ...   78.228149   \n","9988              2018.366254            22860.992562  ...   28.323744   \n","9989              1867.422378           119722.211518  ...   38.801735   \n","\n","      mfcc17_mean  mfcc17_var  mfcc18_mean  mfcc18_var  mfcc19_mean  \\\n","0       -3.241280   36.488243     0.722209   38.099152    -5.050335   \n","1       -6.055294   40.677654     0.159015   51.264091    -2.837699   \n","2       -1.768610   28.348579     2.378768   45.717648    -1.938424   \n","3       -3.841155   28.337118     1.218588   34.770935    -3.580352   \n","4        0.664582   45.880913     1.689446   51.363583    -3.392489   \n","...           ...         ...          ...         ...          ...   \n","9985    -9.094270   38.326839    -4.246976   31.049839    -5.625813   \n","9986   -12.375726   66.418587    -3.081278   54.414265   -11.960546   \n","9987    -2.524483   21.778994     4.809936   25.980829     1.775686   \n","9988    -5.363541   17.209942     6.462601   21.442928     2.354765   \n","9989   -11.598399   58.983097    -0.178517   55.761299    -6.903252   \n","\n","      mfcc19_var  mfcc20_mean  mfcc20_var  label  \n","0      33.618073    -0.243027   43.771767  blues  \n","1      97.030830     5.784063   59.943081  blues  \n","2      53.050835     2.517375   33.105122  blues  \n","3      50.836224     3.630866   32.023678  blues  \n","4      26.738789     0.536961   29.146694  blues  \n","...          ...          ...         ...    ...  \n","9985   48.804092     1.818823   38.966969   rock  \n","9986   63.452255     0.428857   18.697033   rock  \n","9987   48.582378    -0.299545   41.586990   rock  \n","9988   24.843613     0.675824   12.787750   rock  \n","9989   39.485901    -3.412534   31.727489   rock  \n","\n","[9990 rows x 60 columns]"],"text/html":["\n","  <div id=\"df-e007dc91-c650-4a51-ac0c-1d51fc1444a6\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>filename</th>\n","      <th>length</th>\n","      <th>chroma_stft_mean</th>\n","      <th>chroma_stft_var</th>\n","      <th>rms_mean</th>\n","      <th>rms_var</th>\n","      <th>spectral_centroid_mean</th>\n","      <th>spectral_centroid_var</th>\n","      <th>spectral_bandwidth_mean</th>\n","      <th>spectral_bandwidth_var</th>\n","      <th>...</th>\n","      <th>mfcc16_var</th>\n","      <th>mfcc17_mean</th>\n","      <th>mfcc17_var</th>\n","      <th>mfcc18_mean</th>\n","      <th>mfcc18_var</th>\n","      <th>mfcc19_mean</th>\n","      <th>mfcc19_var</th>\n","      <th>mfcc20_mean</th>\n","      <th>mfcc20_var</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>blues.00000.0.wav</td>\n","      <td>66149</td>\n","      <td>0.335406</td>\n","      <td>0.091048</td>\n","      <td>0.130405</td>\n","      <td>0.003521</td>\n","      <td>1773.065032</td>\n","      <td>167541.630869</td>\n","      <td>1972.744388</td>\n","      <td>117335.771563</td>\n","      <td>...</td>\n","      <td>39.687145</td>\n","      <td>-3.241280</td>\n","      <td>36.488243</td>\n","      <td>0.722209</td>\n","      <td>38.099152</td>\n","      <td>-5.050335</td>\n","      <td>33.618073</td>\n","      <td>-0.243027</td>\n","      <td>43.771767</td>\n","      <td>blues</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>blues.00000.1.wav</td>\n","      <td>66149</td>\n","      <td>0.343065</td>\n","      <td>0.086147</td>\n","      <td>0.112699</td>\n","      <td>0.001450</td>\n","      <td>1816.693777</td>\n","      <td>90525.690866</td>\n","      <td>2010.051501</td>\n","      <td>65671.875673</td>\n","      <td>...</td>\n","      <td>64.748276</td>\n","      <td>-6.055294</td>\n","      <td>40.677654</td>\n","      <td>0.159015</td>\n","      <td>51.264091</td>\n","      <td>-2.837699</td>\n","      <td>97.030830</td>\n","      <td>5.784063</td>\n","      <td>59.943081</td>\n","      <td>blues</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>blues.00000.2.wav</td>\n","      <td>66149</td>\n","      <td>0.346815</td>\n","      <td>0.092243</td>\n","      <td>0.132003</td>\n","      <td>0.004620</td>\n","      <td>1788.539719</td>\n","      <td>111407.437613</td>\n","      <td>2084.565132</td>\n","      <td>75124.921716</td>\n","      <td>...</td>\n","      <td>67.336563</td>\n","      <td>-1.768610</td>\n","      <td>28.348579</td>\n","      <td>2.378768</td>\n","      <td>45.717648</td>\n","      <td>-1.938424</td>\n","      <td>53.050835</td>\n","      <td>2.517375</td>\n","      <td>33.105122</td>\n","      <td>blues</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>blues.00000.3.wav</td>\n","      <td>66149</td>\n","      <td>0.363639</td>\n","      <td>0.086856</td>\n","      <td>0.132565</td>\n","      <td>0.002448</td>\n","      <td>1655.289045</td>\n","      <td>111952.284517</td>\n","      <td>1960.039988</td>\n","      <td>82913.639269</td>\n","      <td>...</td>\n","      <td>47.739452</td>\n","      <td>-3.841155</td>\n","      <td>28.337118</td>\n","      <td>1.218588</td>\n","      <td>34.770935</td>\n","      <td>-3.580352</td>\n","      <td>50.836224</td>\n","      <td>3.630866</td>\n","      <td>32.023678</td>\n","      <td>blues</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>blues.00000.4.wav</td>\n","      <td>66149</td>\n","      <td>0.335579</td>\n","      <td>0.088129</td>\n","      <td>0.143289</td>\n","      <td>0.001701</td>\n","      <td>1630.656199</td>\n","      <td>79667.267654</td>\n","      <td>1948.503884</td>\n","      <td>60204.020268</td>\n","      <td>...</td>\n","      <td>30.336359</td>\n","      <td>0.664582</td>\n","      <td>45.880913</td>\n","      <td>1.689446</td>\n","      <td>51.363583</td>\n","      <td>-3.392489</td>\n","      <td>26.738789</td>\n","      <td>0.536961</td>\n","      <td>29.146694</td>\n","      <td>blues</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9985</th>\n","      <td>rock.00099.5.wav</td>\n","      <td>66149</td>\n","      <td>0.349126</td>\n","      <td>0.080515</td>\n","      <td>0.050019</td>\n","      <td>0.000097</td>\n","      <td>1499.083005</td>\n","      <td>164266.886443</td>\n","      <td>1718.707215</td>\n","      <td>85931.574523</td>\n","      <td>...</td>\n","      <td>42.485981</td>\n","      <td>-9.094270</td>\n","      <td>38.326839</td>\n","      <td>-4.246976</td>\n","      <td>31.049839</td>\n","      <td>-5.625813</td>\n","      <td>48.804092</td>\n","      <td>1.818823</td>\n","      <td>38.966969</td>\n","      <td>rock</td>\n","    </tr>\n","    <tr>\n","      <th>9986</th>\n","      <td>rock.00099.6.wav</td>\n","      <td>66149</td>\n","      <td>0.372564</td>\n","      <td>0.082626</td>\n","      <td>0.057897</td>\n","      <td>0.000088</td>\n","      <td>1847.965128</td>\n","      <td>281054.935973</td>\n","      <td>1906.468492</td>\n","      <td>99727.037054</td>\n","      <td>...</td>\n","      <td>32.415203</td>\n","      <td>-12.375726</td>\n","      <td>66.418587</td>\n","      <td>-3.081278</td>\n","      <td>54.414265</td>\n","      <td>-11.960546</td>\n","      <td>63.452255</td>\n","      <td>0.428857</td>\n","      <td>18.697033</td>\n","      <td>rock</td>\n","    </tr>\n","    <tr>\n","      <th>9987</th>\n","      <td>rock.00099.7.wav</td>\n","      <td>66149</td>\n","      <td>0.347481</td>\n","      <td>0.089019</td>\n","      <td>0.052403</td>\n","      <td>0.000701</td>\n","      <td>1346.157659</td>\n","      <td>662956.246325</td>\n","      <td>1561.859087</td>\n","      <td>138762.841945</td>\n","      <td>...</td>\n","      <td>78.228149</td>\n","      <td>-2.524483</td>\n","      <td>21.778994</td>\n","      <td>4.809936</td>\n","      <td>25.980829</td>\n","      <td>1.775686</td>\n","      <td>48.582378</td>\n","      <td>-0.299545</td>\n","      <td>41.586990</td>\n","      <td>rock</td>\n","    </tr>\n","    <tr>\n","      <th>9988</th>\n","      <td>rock.00099.8.wav</td>\n","      <td>66149</td>\n","      <td>0.387527</td>\n","      <td>0.084815</td>\n","      <td>0.066430</td>\n","      <td>0.000320</td>\n","      <td>2084.515327</td>\n","      <td>203891.039161</td>\n","      <td>2018.366254</td>\n","      <td>22860.992562</td>\n","      <td>...</td>\n","      <td>28.323744</td>\n","      <td>-5.363541</td>\n","      <td>17.209942</td>\n","      <td>6.462601</td>\n","      <td>21.442928</td>\n","      <td>2.354765</td>\n","      <td>24.843613</td>\n","      <td>0.675824</td>\n","      <td>12.787750</td>\n","      <td>rock</td>\n","    </tr>\n","    <tr>\n","      <th>9989</th>\n","      <td>rock.00099.9.wav</td>\n","      <td>66149</td>\n","      <td>0.369293</td>\n","      <td>0.086759</td>\n","      <td>0.050524</td>\n","      <td>0.000067</td>\n","      <td>1634.330126</td>\n","      <td>411429.169769</td>\n","      <td>1867.422378</td>\n","      <td>119722.211518</td>\n","      <td>...</td>\n","      <td>38.801735</td>\n","      <td>-11.598399</td>\n","      <td>58.983097</td>\n","      <td>-0.178517</td>\n","      <td>55.761299</td>\n","      <td>-6.903252</td>\n","      <td>39.485901</td>\n","      <td>-3.412534</td>\n","      <td>31.727489</td>\n","      <td>rock</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>9990 rows Ã— 60 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e007dc91-c650-4a51-ac0c-1d51fc1444a6')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e007dc91-c650-4a51-ac0c-1d51fc1444a6 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e007dc91-c650-4a51-ac0c-1d51fc1444a6');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":24}],"source":["from sklearn.ensemble import BaggingClassifier\n","import pandas as pd\n","from pandas import DataFrame, Series\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn import tree\n","from sklearn.metrics import accuracy_score \n","music = pd.read_csv('https://raw.githubusercontent.com/zacharski/ml-class/master/data/gtzan.csv')\n","music"]},{"cell_type":"markdown","metadata":{"id":"yAAvRO8JWb6h"},"source":["Let's examine the values of the label column (the genres):"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"0m3K-MgiWb6h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679282576950,"user_tz":240,"elapsed":129,"user":{"displayName":"stephano montes","userId":"15446985780563685802"}},"outputId":"2568921f-7010-45bd-baf1-b3f114b22bb2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz',\n","       'metal', 'pop', 'reggae', 'rock'], dtype=object)"]},"metadata":{},"execution_count":14}],"source":["music.label.unique()"]},{"cell_type":"markdown","metadata":{"id":"AlKrZfoVWb6h"},"source":["Those are the 10 genres we are trying to predict. So if we were just to guess without hearing the clip, we would be accurate 10% of the time. How accurate do you think you would be based on hearing a 3 second clip? I am pretty confident I could correctly label the 30 second clips, but I am much less confident about labeling 3 second ones. Since guessing randomly would give me 10% accuracy, I am estimating maybe 50-60% accuracy. Let's see how a computer does.\n","\n","#### Feature Names\n","So the column we are trying to predict is `label`. Now let's get the names of the feature columns"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"D1bvL8bCWb6h","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679283452611,"user_tz":240,"elapsed":104,"user":{"displayName":"stephano montes","userId":"15446985780563685802"}},"outputId":"a4dccdef-9f58-492b-f115-20079daa96c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["['length', 'chroma_stft_mean', 'chroma_stft_var', 'rms_mean', 'rms_var', 'spectral_centroid_mean', 'spectral_centroid_var', 'spectral_bandwidth_mean', 'spectral_bandwidth_var', 'rolloff_mean', 'rolloff_var', 'zero_crossing_rate_mean', 'zero_crossing_rate_var', 'harmony_mean', 'harmony_var', 'perceptr_mean', 'perceptr_var', 'tempo', 'mfcc1_mean', 'mfcc1_var', 'mfcc2_mean', 'mfcc2_var', 'mfcc3_mean', 'mfcc3_var', 'mfcc4_mean', 'mfcc4_var', 'mfcc5_mean', 'mfcc5_var', 'mfcc6_mean', 'mfcc6_var', 'mfcc7_mean', 'mfcc7_var', 'mfcc8_mean', 'mfcc8_var', 'mfcc9_mean', 'mfcc9_var', 'mfcc10_mean', 'mfcc10_var', 'mfcc11_mean', 'mfcc11_var', 'mfcc12_mean', 'mfcc12_var', 'mfcc13_mean', 'mfcc13_var', 'mfcc14_mean', 'mfcc14_var', 'mfcc15_mean', 'mfcc15_var', 'mfcc16_mean', 'mfcc16_var', 'mfcc17_mean', 'mfcc17_var', 'mfcc18_mean', 'mfcc18_var', 'mfcc19_mean', 'mfcc19_var', 'mfcc20_mean', 'mfcc20_var']\n","58\n"]}],"source":["featureNames = list(music.columns)\n","featureNames.remove('filename')\n","featureNames.remove('label')\n","print(featureNames)\n","print(len(featureNames))"]},{"cell_type":"markdown","metadata":{"id":"I-B5g-RAWb6h"},"source":["So we have 58 features. \n","\n","#### Training and test sets\n","Now it is time to construct the training and test sets:"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"1zqluOErWb6h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679283451788,"user_tz":240,"elapsed":158,"user":{"displayName":"stephano montes","userId":"15446985780563685802"}},"outputId":"980aba9f-8011-41f8-cb60-ce4e30903b45"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["4801       hiphop\n","7644          pop\n","3226        disco\n","3667        disco\n","1526    classical\n","          ...    \n","2369      country\n","5684         jazz\n","3055        disco\n","2842      country\n","6765        metal\n","Name: label, Length: 1998, dtype: object"]},"metadata":{},"execution_count":25}],"source":["## divide the original data 80% going into the music_training dataset \n","## the rest in music_test\n","music_training, music_test = train_test_split(music, train_size=0.8)\n","                              \n","## now create the DataFrames for just the features (excluding the label column \n","## filename column)                              \n","music_training_features = music_training.drop(columns=['label', 'filename'])\n","music_test_features = music_test.drop(columns=['label', 'filename'])\n","                              \n","## now create the labels data structure for both the training and test sets                              \n","music_training_labels = music_training['label']\n","music_test_labels = music_test['label']\n","music_test_labels"]},{"cell_type":"markdown","metadata":{"id":"touv2_GAWb6h"},"source":["### Building a single decision tree classifier\n","Let's build a single decision tree classifier called `clf` using entropy, fit it to the data, make predictions and determine the accuracy:"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"1V3O5patWb6h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679282866490,"user_tz":240,"elapsed":1380,"user":{"displayName":"stephano montes","userId":"15446985780563685802"}},"outputId":"037aad21-e6fc-4f02-9bb3-13183396ab1e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6536536536536537"]},"metadata":{},"execution_count":18}],"source":["## Create clf, an instance of the Decision Tree Classifier\n","clf = tree.DecisionTreeClassifier(criterion='entropy')\n","\n","\n","## Fit it to the data\n","clf.fit(music_training_features, music_training_labels)\n","\n","## get the predictions for the test set\n","basePredictions = clf.predict(music_test_features)\n","\n","## get the accuracy score\n","baseAccuracy = accuracy_score(music_test_labels, basePredictions)\n","baseAccuracy"]},{"cell_type":"markdown","metadata":{"id":"DEcDPHNUWb6h"},"source":["When I did this I got 66% accuracy. That doesn't sound great but keep in mind that random guessing would only be 10% accuracy. \n","\n","### Building a Random Patch Classifier\n","\n","Now we are going to build a random patch classifier.\n","\n","* the base classifier will be a decision tree using entropy\n","* the ensemble will contain 20 base classifiers\n","* each classifier will use a random sample of 70% of the training data\n","* each classifier will use a random sample of 70% of the features\n","* the sampling will be done with replacement\n","* it will use all available cpu cores.\n","\n","We are going to\n","\n","1. build the classifier\n","2. train the classifier on the data\n","3. make predictions on the test set\n","4. determine the accuracy"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"HjTNhPVdWb6h","executionInfo":{"status":"ok","timestamp":1679283054943,"user_tz":240,"elapsed":2,"user":{"displayName":"stephano montes","userId":"15446985780563685802"}}},"outputs":[],"source":["clf1 = tree.DecisionTreeClassifier(criterion='entropy')\n","Clf = BaggingClassifier(clf1, n_estimators=20, max_samples=0.7, max_features=0.7, bootstrap=True, n_jobs=-1)"]},{"cell_type":"markdown","metadata":{"id":"7W3cc2--Wb6h"},"source":["What accuracy did you get? Was it better than using a single classifier?\n","Keep your original code above. Make a copy of it below and \n","experiment a bit with the hyperparameters. (try 3 or 4 different things) What is the best accuracy you can get?"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"p83tIBVhWb6i","colab":{"base_uri":"https://localhost:8080/","height":118},"executionInfo":{"status":"ok","timestamp":1679283274120,"user_tz":240,"elapsed":6615,"user":{"displayName":"stephano montes","userId":"15446985780563685802"}},"outputId":"fa243111-5dd8-44d9-9b30-719b10bea783"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["BaggingClassifier(estimator=DecisionTreeClassifier(criterion='entropy'),\n","                  max_features=0.7, max_samples=0.7, n_estimators=20,\n","                  n_jobs=-1)"],"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BaggingClassifier(estimator=DecisionTreeClassifier(criterion=&#x27;entropy&#x27;),\n","                  max_features=0.7, max_samples=0.7, n_estimators=20,\n","                  n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BaggingClassifier</label><div class=\"sk-toggleable__content\"><pre>BaggingClassifier(estimator=DecisionTreeClassifier(criterion=&#x27;entropy&#x27;),\n","                  max_features=0.7, max_samples=0.7, n_estimators=20,\n","                  n_jobs=-1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;)</pre></div></div></div></div></div></div></div></div></div></div>"]},"metadata":{},"execution_count":22}],"source":["Clf.fit(music_training_features, music_training_labels)"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"FkuEN3GUWb6i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679283330757,"user_tz":240,"elapsed":600,"user":{"displayName":"stephano montes","userId":"15446985780563685802"}},"outputId":"033c32ce-57aa-47ee-9e9a-59a72fecd0b0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8383383383383384"]},"metadata":{},"execution_count":23}],"source":["Predictions = Clf.predict(music_test_features)\n","Accuracy = accuracy_score(music_test_labels, Predictions)\n","Accuracy"]},{"cell_type":"markdown","metadata":{"id":"MP2mAHhTWb6i"},"source":["\n","##<font color='#52BE80'>NOTES</font>\n","\n","### Breast Cancer Database\n","\n","[back](#Wisconsin-Cancer-Dataset)\n","\n","  This breast cancer databases was obtained from the University of Wisconsin\n","   Hospitals, Madison from Dr. William H. Wolberg.  If you publish results\n","   when using this database, then please include this information in your\n","   acknowledgements.  Also, please cite one or more of:\n","\n","   1. O. L. Mangasarian and W. H. Wolberg: \"Cancer diagnosis via linear \n","      programming\", SIAM News, Volume 23, Number 5, September 1990, pp 1 & 18.\n","\n","   2. William H. Wolberg and O.L. Mangasarian: \"Multisurface method of \n","      pattern separation for medical diagnosis applied to breast cytology\", \n","      Proceedings of the National Academy of Sciences, U.S.A., Volume 87, \n","      December 1990, pp 9193-9196.\n","\n","   3. O. L. Mangasarian, R. Setiono, and W.H. Wolberg: \"Pattern recognition \n","      via linear programming: Theory and application to medical diagnosis\", \n","      in: \"Large-scale numerical optimization\", Thomas F. Coleman and Yuying\n","      Li, editors, SIAM Publications, Philadelphia 1990, pp 22-30.\n","\n","   4. K. P. Bennett & O. L. Mangasarian: \"Robust linear programming \n","      discrimination of two linearly inseparable sets\", Optimization Methods\n","      and Software 1, 1992, 23-34 (Gordon & Breach Science Publishers).\n","\n","Title: Wisconsin Breast Cancer Database (January 8, 1991)\n","\n","\n","Sources:\n","   -- Dr. WIlliam H. Wolberg (physician)\n","      University of Wisconsin Hospitals\n","      Madison, Wisconsin\n","      USA\n","   -- Donor: Olvi Mangasarian (mangasarian@cs.wisc.edu)\n","      Received by David W. Aha (aha@cs.jhu.edu)\n","   -- Date: 15 July 1992"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z4yT3Ci2Wb6i"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/zacharski/ml-class/blob/master/labs/bagging_n_pasting.ipynb","timestamp":1679268201588}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}